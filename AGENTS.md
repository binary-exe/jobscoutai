# JobScout Development Guide for AI Agents

This document provides comprehensive context for AI agents (like Codex CLI) to continue development on JobScout.

**âš ï¸ CRITICAL: Read this entire document before making any changes. This file is re-read every session.**

## ðŸ“‹ Table of Contents

1. [Quick Start & Commands](#quick-start--commands) âš¡ **START HERE**
2. [Repository Map](#repository-map)
3. [Definition of Done](#definition-of-done)
4. [Development Constraints](#development-constraints)
5. [Project Overview](#project-overview)
6. [Architecture](#architecture)
7. [Directory Structure](#directory-structure)
8. [Key Components](#key-components)
9. [API Endpoints](#api-endpoints)
10. [Database Schema](#database-schema)
11. [Environment Variables](#environment-variables)
12. [Development Workflow](#development-workflow)
13. [Common Tasks](#common-tasks)
14. [Deployment](#deployment)
15. [Known Issues & TODOs](#known-issues--todos)

---

## Quick Start & Commands

### Repository Map

```
jobscout/                          # Root monorepo
â”œâ”€â”€ frontend/                      # Next.js 14 app (Vercel)
â”‚   â”œâ”€â”€ app/                      # Pages (App Router)
â”‚   â”œâ”€â”€ components/               # React components
â”‚   â”œâ”€â”€ lib/                      # API client, utils
â”‚   â””â”€â”€ package.json              # npm scripts
â”‚
â”œâ”€â”€ backend/                       # FastAPI app (Fly.io)
â”‚   â”œâ”€â”€ app/                      # FastAPI application
â”‚   â”‚   â”œâ”€â”€ api/                  # REST endpoints
â”‚   â”‚   â”œâ”€â”€ core/                 # Config, database
â”‚   â”‚   â”œâ”€â”€ storage/              # Postgres adapter
â”‚   â”‚   â””â”€â”€ worker.py             # Background tasks
â”‚   â”œâ”€â”€ Dockerfile                # Fly.io deployment
â”‚   â””â”€â”€ requirements.txt          # Python deps
â”‚
â””â”€â”€ jobscout/                     # Core scraping library (Python package)
    â”œâ”€â”€ providers/                # Job source providers
    â”œâ”€â”€ fetchers/                 # HTTP/browser fetching
    â”œâ”€â”€ extract/                  # HTML/JSON extraction
    â”œâ”€â”€ llm/                      # AI features
    â””â”€â”€ storage/                  # SQLite adapter
```

### Exact Commands

#### Development

**Backend (FastAPI):**
```bash
cd backend
pip install -r requirements.txt
pip install -e ../  # Install jobscout package in dev mode
cp env.sample .env
# Edit .env with your settings
uvicorn backend.app.main:app --reload --host 0.0.0.0 --port 8000
```
- Runs at: `http://localhost:8000`
- API docs: `http://localhost:8000/docs`

**Frontend (Next.js):**
```bash
cd frontend
npm install
cp env.sample .env.local
# Edit .env.local: NEXT_PUBLIC_API_URL=http://localhost:8000/api/v1
npm run dev
```
- Runs at: `http://localhost:3000`

**Core Library (CLI):**
```bash
# Install in dev mode
pip install -e ".[all]"

# Run scrape
python -m jobscout "automation engineer" --remote-only --verbose
```

#### Testing

**Backend:**
```bash
cd backend
# No formal test suite yet - test via Swagger UI at /docs
# Or manual API calls:
curl http://localhost:8000/api/v1/jobs?q=test
```

**Frontend:**
```bash
cd frontend
npm run lint        # TypeScript/ESLint checks
npm run build       # Production build test
```

**Core Library:**
```bash
# Manual testing via CLI
python -m jobscout "test query" --verbose
```

#### Build

**Backend:**
```bash
cd backend
# Build happens in Dockerfile for Fly.io
# Local build test:
docker build -f Dockerfile -t jobscout-api .
```

**Frontend:**
```bash
cd frontend
npm run build       # Production build
npm run start       # Test production build locally
```

#### Deploy

**Backend (Fly.io):**
```bash
# From repo root
fly deploy -a jobscout-api

# Check logs
fly logs -a jobscout-api

# Set secrets (if needed)
fly secrets set JOBSCOUT_DATABASE_URL="..." -a jobscout-api
```

**Frontend (Vercel):**
```bash
# Auto-deploys on push to main branch
# Manual deploy:
cd frontend
npx vercel --prod

# Or via GitHub integration (automatic)
```

**Database (Supabase):**
- Schema changes: Run SQL in Supabase SQL Editor
- Connection: Use Session pooler connection string (not Direct)

---

## Definition of Done

Before considering any task complete, verify:

### âœ… Code Quality
- [ ] **Linting**: No lint errors
  - Backend: Python code follows PEP 8 (no formal linter yet, but check manually)
  - Frontend: `npm run lint` passes
- [ ] **Type Safety**: TypeScript types are correct (frontend)
- [ ] **No Console Errors**: Check browser console and server logs

### âœ… Functionality
- [ ] **Local Testing**: Feature works in local dev environment
  - Backend: Test via Swagger UI (`/docs`) or curl
  - Frontend: Test in browser at `http://localhost:3000`
- [ ] **API Compatibility**: Changes don't break existing API contracts
- [ ] **Database**: Schema changes (if any) are backward compatible

### âœ… Deployment Check
- [ ] **Build Success**: 
  - Frontend: `npm run build` succeeds
  - Backend: Docker build succeeds (or would succeed)
- [ ] **Environment Variables**: All required env vars documented in `env.sample`
- [ ] **Documentation**: Updated `AGENTS.md` if architecture/commands changed

### âœ… Git Hygiene
- [ ] **Commits**: Clear, descriptive commit messages
- [ ] **No Secrets**: No API keys, tokens, or passwords in code
- [ ] **No Large Files**: No binary files or large data files committed

**Note**: Formal unit tests are not yet implemented. Manual testing via Swagger UI and browser is acceptable for now.

---

## Development Constraints

### ðŸš« DO NOT

1. **Infrastructure Changes**
   - âŒ Do NOT change deployment platforms (Fly.io, Vercel, Supabase)
   - âŒ Do NOT modify `fly.toml` structure without explicit request
   - âŒ Do NOT change Dockerfile base image or major dependencies
   - âŒ Do NOT add new infrastructure services (Redis, queues, etc.) without approval

2. **Dependency Management**
   - âŒ Do NOT add new Python packages to `backend/requirements.txt` without checking if they're in `pyproject.toml`
   - âŒ Do NOT add new npm packages to `frontend/package.json` without justification
   - âŒ Do NOT upgrade major versions (e.g., Next.js 14 â†’ 15, Python 3.11 â†’ 3.12) without approval
   - âœ… Prefer using existing dependencies from `pyproject.toml` optional groups

3. **Database Schema**
   - âŒ Do NOT drop or rename existing columns (breaking changes)
   - âŒ Do NOT change column types without migration plan
   - âœ… Add new columns as nullable when possible
   - âœ… Always update both `backend/app/storage/postgres.py` schema AND `AGENTS.md` documentation

4. **API Contracts**
   - âŒ Do NOT remove or rename existing API endpoints
   - âŒ Do NOT change request/response schemas without versioning
   - âœ… Add new endpoints as `/api/v1/new-endpoint`
   - âœ… Maintain backward compatibility

5. **Environment Variables**
   - âŒ Do NOT remove existing env vars
   - âŒ Do NOT change env var names without migration path
   - âœ… Always update `env.sample` files when adding new vars
   - âœ… Document new vars in `AGENTS.md`

6. **File Structure**
   - âŒ Do NOT move files between `frontend/` and `backend/` without explicit request
   - âŒ Do NOT restructure the `jobscout/` package without approval
   - âœ… Follow existing patterns and conventions

### âœ… DO

1. **Follow Existing Patterns**
   - Use existing component patterns in `frontend/components/`
   - Follow API endpoint structure in `backend/app/api/`
   - Match provider implementation style in `jobscout/providers/`

2. **Error Handling**
   - Always handle errors gracefully
   - Log errors with context
   - Return appropriate HTTP status codes

3. **Documentation**
   - Update `AGENTS.md` if you add new commands, endpoints, or patterns
   - Add comments for complex logic
   - Update `env.sample` files for new configuration

4. **Testing**
   - Test locally before committing
   - Verify API endpoints via Swagger UI
   - Check frontend in browser

5. **Cost Awareness**
   - Be mindful of API costs (OpenAI, external APIs)
   - Use caching where appropriate
   - Respect rate limits

---

## Project Overview

**JobScout** is an AI-powered job aggregator that:
- Scrapes jobs from multiple sources (RemoteOK, WeWorkRemotely, Remotive, Arbeitnow, etc.)
- Provides on-demand scraping via web UI
- Uses AI (OpenAI GPT-4o-mini) for ranking, classification, and enrichment (optional)
- Stores jobs in PostgreSQL (Supabase) or SQLite (local dev)
- Serves a beautiful Next.js frontend with real-time search

**Tech Stack:**
- **Backend**: FastAPI (Python 3.11), asyncpg, APScheduler
- **Frontend**: Next.js 14, React 18, TypeScript, Tailwind CSS
- **Database**: PostgreSQL (Supabase) / SQLite (dev)
- **Deployment**: Fly.io (backend), Vercel (frontend)
- **AI**: OpenAI API (gpt-4o-mini, optional)

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (Next.js)                         â”‚
â”‚              Vercel: jobscoutai.vercel.app                    â”‚
â”‚  - Server-side rendering                                      â”‚
â”‚  - On-demand scraping via POST /api/v1/scrape                â”‚
â”‚  - Real-time job search with filters                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTPS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Backend (FastAPI)                            â”‚
â”‚            Fly.io: jobscout-api.fly.dev                       â”‚
â”‚  - REST API (/api/v1/jobs, /api/v1/scrape, /api/v1/runs)     â”‚
â”‚  - Background scraping (async, writes to Postgres)            â”‚
â”‚  - Scheduled scrapes (APScheduler, every 6h)                  â”‚
â”‚  - Rate limiting & concurrency caps                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                â”‚                â”‚
          â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Supabase      â”‚ â”‚   OpenAI    â”‚ â”‚  Job Sources    â”‚
â”‚   PostgreSQL    â”‚ â”‚  (Optional) â”‚ â”‚  (APIs/RSS)     â”‚
â”‚   - jobs table  â”‚ â”‚ gpt-4o-mini â”‚ â”‚  - RemoteOK     â”‚
â”‚   - runs table  â”‚ â”‚             â”‚ â”‚  - WeWorkRemotelyâ”‚
â”‚   - llm_cache   â”‚ â”‚             â”‚ â”‚  - Remotive     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow:**
1. User enters search query â†’ Frontend triggers `POST /api/v1/scrape`
2. Backend enqueues scrape run â†’ Returns `run_id` immediately
3. Background worker scrapes from multiple providers â†’ Writes to Postgres
4. Frontend polls `GET /api/v1/runs/{run_id}` â†’ Shows progress
5. When complete, frontend refreshes job list from `GET /api/v1/jobs`

---

## Directory Structure

```
jobscout/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/                # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py         # GET /jobs, GET /jobs/{id}
â”‚   â”‚   â”‚   â”œâ”€â”€ scrape.py       # POST /scrape (public, rate-limited)
â”‚   â”‚   â”‚   â”œâ”€â”€ runs.py          # GET /runs/{id}, GET /runs/latest
â”‚   â”‚   â”‚   â””â”€â”€ admin.py        # POST /admin/run (admin-only)
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py       # Settings (Pydantic)
â”‚   â”‚   â”‚   â””â”€â”€ database.py     # asyncpg connection pool
â”‚   â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”‚   â””â”€â”€ postgres.py     # Postgres adapter (upsert_job, start_run, etc.)
â”‚   â”‚   â”œâ”€â”€ worker.py           # Background scraping logic
â”‚   â”‚   â””â”€â”€ main.py             # FastAPI app entry point
â”‚   â”œâ”€â”€ Dockerfile              # Fly.io deployment
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ env.sample              # Environment variable template
â”‚
â”œâ”€â”€ frontend/                   # Next.js frontend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx            # Home page (job list + search)
â”‚   â”‚   â”œâ”€â”€ job/[id]/page.tsx   # Job detail page
â”‚   â”‚   â””â”€â”€ layout.tsx          # Root layout
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ SearchBar.tsx       # Search input + scrape trigger
â”‚   â”‚   â”œâ”€â”€ JobCard.tsx         # Job list item
â”‚   â”‚   â”œâ”€â”€ Filters.tsx         # Sidebar filters
â”‚   â”‚   â”œâ”€â”€ FormattedDescription.tsx  # Formats job descriptions
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ api.ts              # API client functions
â”‚   â”‚   â””â”€â”€ utils.ts            # Utility functions
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ jobscout/                   # Core scraping library
â”‚   â”œâ”€â”€ orchestrator.py         # Main scrape orchestration
â”‚   â”œâ”€â”€ models.py              # Criteria, NormalizedJob, enums
â”‚   â”œâ”€â”€ dedupe.py              # Multi-layer deduplication
â”‚   â”œâ”€â”€ providers/             # Job source providers
â”‚   â”‚   â”œâ”€â”€ remoteok.py
â”‚   â”‚   â”œâ”€â”€ weworkremotely.py
â”‚   â”‚   â”œâ”€â”€ remotive.py
â”‚   â”‚   â”œâ”€â”€ arbeitnow.py
â”‚   â”‚   â”œâ”€â”€ discovery.py        # Auto-discovers ATS job boards
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ fetchers/              # HTTP/Browser fetching
â”‚   â”‚   â”œâ”€â”€ http.py            # aiohttp with retries/throttling
â”‚   â”‚   â””â”€â”€ browser.py         # Playwright (optional)
â”‚   â”œâ”€â”€ extract/               # Data extraction
â”‚   â”‚   â”œâ”€â”€ html.py            # HTML stripping/parsing
â”‚   â”‚   â”œâ”€â”€ jsonld.py          # Schema.org JobPosting parsing
â”‚   â”‚   â””â”€â”€ enrich.py          # Company page enrichment
â”‚   â”œâ”€â”€ llm/                   # AI features (optional)
â”‚   â”‚   â”œâ”€â”€ provider.py        # Abstract LLM client
â”‚   â”‚   â”œâ”€â”€ openai_client.py   # OpenAI implementation
â”‚   â”‚   â”œâ”€â”€ rank.py            # Job ranking
â”‚   â”‚   â”œâ”€â”€ classify.py        # Remote type, employment type
â”‚   â”‚   â”œâ”€â”€ enrich_llm.py      # Summary, requirements, tech stack
â”‚   â”‚   â”œâ”€â”€ company_agent.py   # Company research
â”‚   â”‚   â”œâ”€â”€ alerts.py          # Quality/safety flags
â”‚   â”‚   â””â”€â”€ cache.py           # SQLite LLM response cache
â”‚   â””â”€â”€ storage/
â”‚       â””â”€â”€ sqlite.py          # SQLite adapter (local dev)
â”‚
â”œâ”€â”€ pyproject.toml             # Python package config
â”œâ”€â”€ README.md
â”œâ”€â”€ DEPLOY.md
â””â”€â”€ AGENTS.md                  # This file
```

---

## Key Components

### Backend

#### `backend/app/worker.py`
- `enqueue_scrape_run()`: Creates run record, triggers background scrape
- `trigger_scrape_run()`: Synchronous scrape (used by admin endpoint)
- `run_scheduled_scrape()`: Scheduled scrape runner

#### `backend/app/api/scrape.py`
- `POST /api/v1/scrape`: Public endpoint for on-demand scraping
  - Rate limiting: 6 requests/hour per IP
  - Concurrency cap: 1 active scrape
  - Returns `{status: "queued", run_id: N}` immediately

#### `backend/app/api/runs.py`
- `GET /api/v1/runs/{run_id}`: Get run status and stats
- `GET /api/v1/runs/latest`: Get most recent run

#### `backend/app/storage/postgres.py`
- `upsert_job_from_dict()`: Insert/update job (handles timestamps)
- `start_run()`: Create run record
- `finish_run()`: Update run with final stats

### Frontend

#### `frontend/components/SearchBar.tsx`
- Search input with Enter key handler
- Triggers `POST /api/v1/scrape` on submit
- Polls run status, shows "Scraping..." indicator
- Auto-refreshes results when scrape completes
- AI toggle (sparkles icon, default off)

#### `frontend/components/FormattedDescription.tsx`
- Formats plain text job descriptions
- Detects headings, bullet points, paragraphs
- Renders with proper spacing and structure

#### `frontend/lib/api.ts`
- `getJobs()`: Fetch jobs with filters
- `getJob(id)`: Fetch single job details
- `scrapeNow()`: Trigger on-demand scrape
- `getRunStatus(id)`: Poll run status

### Core Library

#### `jobscout/orchestrator.py`
- `run_scrape()`: Main orchestration function
  - Discovery (optional)
  - Provider collection (parallel)
  - Filtering
  - Deduplication
  - Enrichment
  - AI pipeline (optional)
  - Storage

#### `jobscout/providers/base.py`
- `Provider` abstract base class
- `ProviderStats` for tracking errors/collected jobs

#### `jobscout/dedupe.py`
- `DedupeEngine`: Multi-layer deduplication
  - Provider ID matching
  - URL canonicalization
  - Fuzzy matching (title + company)
  - LLM arbitration for uncertain pairs (optional)

---

## API Endpoints

### Public Endpoints

#### `GET /api/v1/jobs`
Query parameters:
- `q`: Search query (title, company, description)
- `location`: Location filter
- `remote`: `remote`, `hybrid`, `onsite`
- `employment`: `full_time`, `contract`, etc.
- `source`: Source filter
- `posted_since`: Days ago
- `min_score`: Minimum AI score (0-100)
- `sort`: `ai_score`, `posted_at`, `first_seen_at`
- `page`: Page number (default: 1)
- `page_size`: Items per page (default: 20, max: 50)

Response:
```json
{
  "jobs": [...],
  "total": 188,
  "page": 1,
  "page_size": 20,
  "has_more": true
}
```

#### `GET /api/v1/jobs/{job_id}`
Returns full job details including full description text.

#### `POST /api/v1/scrape`
Request body:
```json
{
  "query": "automation engineer",
  "location": "Remote",
  "use_ai": false
}
```

Response:
```json
{
  "status": "queued",
  "run_id": 2,
  "message": "Scrape queued"
}
```

Rate limits:
- 6 requests/hour per IP
- 1 concurrent scrape max

#### `GET /api/v1/runs/{run_id}`
Response:
```json
{
  "run_id": 2,
  "started_at": "2026-01-09T10:43:06Z",
  "finished_at": null,
  "jobs_collected": 192,
  "jobs_new": 19,
  "jobs_updated": 75,
  "jobs_filtered": 23,
  "errors": 0,
  "sources": "remotive, remoteok, arbeitnow, weworkremotely",
  "criteria": {...}
}
```

#### `GET /api/v1/runs/latest`
Returns the most recent run.

#### `GET /api/v1/admin/stats`
Public stats endpoint (no auth required).

### Admin Endpoints

#### `POST /api/v1/admin/run`
Requires `Authorization: Bearer {admin_token}` header.

Request body:
```json
{
  "query": "automation engineer",
  "location": "Remote",
  "use_ai": false
}
```

---

## Database Schema

### `jobs` table (PostgreSQL)

```sql
CREATE TABLE jobs (
    job_id TEXT PRIMARY KEY,              -- MD5 hash of provider_id + source
    provider_id TEXT,
    source TEXT NOT NULL,
    source_url TEXT,
    
    title TEXT NOT NULL,
    title_normalized TEXT,
    company TEXT NOT NULL,
    company_normalized TEXT,
    
    location_raw TEXT,
    country TEXT,
    city TEXT,
    remote_type TEXT DEFAULT 'unknown',   -- 'remote', 'hybrid', 'onsite', 'unknown'
    employment_types TEXT[] DEFAULT '{}', -- ['full_time', 'contract', ...]
    
    salary_min REAL,
    salary_max REAL,
    salary_currency TEXT,
    
    job_url TEXT,
    job_url_canonical TEXT,
    apply_url TEXT,
    
    description_text TEXT,
    
    emails TEXT[] DEFAULT '{}',
    company_website TEXT,
    linkedin_url TEXT,
    twitter_url TEXT,
    facebook_url TEXT,
    instagram_url TEXT,
    youtube_url TEXT,
    other_urls TEXT[] DEFAULT '{}',
    
    tags TEXT[] DEFAULT '{}',
    founder TEXT,
    
    posted_at TIMESTAMPTZ,
    expires_at TIMESTAMPTZ,
    first_seen_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    last_seen_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    -- AI fields (nullable)
    ai_score REAL,                        -- 0-100 relevance score
    ai_reasons TEXT,
    ai_remote_type TEXT,
    ai_employment_types TEXT[] DEFAULT '{}',
    ai_seniority TEXT,
    ai_confidence REAL,
    ai_summary TEXT,
    ai_requirements TEXT,
    ai_tech_stack TEXT,
    ai_company_domain TEXT,
    ai_company_summary TEXT,
    ai_flags TEXT[] DEFAULT '{}'
);

CREATE INDEX idx_jobs_source ON jobs(source);
CREATE INDEX idx_jobs_company ON jobs(company_normalized);
CREATE INDEX idx_jobs_remote_type ON jobs(remote_type);
CREATE INDEX idx_jobs_posted_at ON jobs(posted_at DESC NULLS LAST);
CREATE INDEX idx_jobs_first_seen ON jobs(first_seen_at DESC);
CREATE INDEX idx_jobs_ai_score ON jobs(ai_score DESC NULLS LAST);
CREATE INDEX idx_jobs_search ON jobs USING gin(to_tsvector('english', title || ' ' || company || ' ' || COALESCE(description_text, '')));
```

### `runs` table

```sql
CREATE TABLE runs (
    run_id SERIAL PRIMARY KEY,
    started_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    finished_at TIMESTAMPTZ,
    criteria JSONB,                       -- Search criteria used
    jobs_collected INTEGER DEFAULT 0,
    jobs_new INTEGER DEFAULT 0,
    jobs_updated INTEGER DEFAULT 0,
    jobs_filtered INTEGER DEFAULT 0,
    errors INTEGER DEFAULT 0,
    sources TEXT,                         -- Comma-separated list
    error_summary TEXT
);

CREATE INDEX idx_runs_started_at ON runs(started_at DESC);
```

### `llm_cache` table (SQLite only, for LLM response caching)

```sql
CREATE TABLE llm_cache (
    key TEXT PRIMARY KEY,                 -- Hash of prompt
    response TEXT NOT NULL,
    created_at TEXT NOT NULL
);
```

---

## Environment Variables

### Backend (`backend/.env` or Fly.io secrets)

```bash
# Database
JOBSCOUT_DATABASE_URL=postgresql://user:pass@host:port/db?sslmode=require
JOBSCOUT_USE_SQLITE=false
JOBSCOUT_SQLITE_PATH=jobs.db

# CORS (JSON array string)
JOBSCOUT_CORS_ORIGINS='["https://jobscoutai.vercel.app","http://localhost:3000"]'

# Admin
JOBSCOUT_ADMIN_TOKEN=your-long-random-token-here

# Scraper defaults
JOBSCOUT_DEFAULT_SEARCH_QUERY=automation engineer
JOBSCOUT_DEFAULT_LOCATION=Remote
JOBSCOUT_SCRAPE_INTERVAL_HOURS=6

# AI (optional)
JOBSCOUT_OPENAI_API_KEY=sk-...
JOBSCOUT_OPENAI_MODEL=gpt-4o-mini
JOBSCOUT_AI_ENABLED=false
JOBSCOUT_AI_MAX_JOBS=50

# Public scrape guardrails
JOBSCOUT_PUBLIC_SCRAPE_ENABLED=true
JOBSCOUT_PUBLIC_SCRAPE_MAX_CONCURRENT=1
JOBSCOUT_PUBLIC_SCRAPE_RATE_LIMIT_PER_HOUR=6
JOBSCOUT_PUBLIC_SCRAPE_MAX_RESULTS_PER_SOURCE=200
```

### Frontend (`frontend/.env.local` or Vercel env vars)

```bash
NEXT_PUBLIC_API_URL=https://jobscout-api.fly.dev/api/v1
```

---

## Development Workflow

> **Note**: See [Quick Start & Commands](#quick-start--commands) for exact commands.

### Local Setup

See [Quick Start & Commands](#quick-start--commands) section above for step-by-step setup instructions.

**Key points:**
- Backend runs on `http://localhost:8000`
- Frontend runs on `http://localhost:3000`
- For local dev, use SQLite: set `JOBSCOUT_USE_SQLITE=true` in backend `.env`
- SQLite DB created at `JOBSCOUT_SQLITE_PATH` (default: `jobs.db`)

### Testing Changes

1. **Backend API:**
   - Visit `http://localhost:8000/docs` for Swagger UI
   - Test endpoints interactively
   - Check logs for errors

2. **Frontend:**
   - Visit `http://localhost:3000`
   - Test in browser with DevTools open
   - Run `npm run lint` before committing
   - Run `npm run build` to verify production build

3. **CLI (core library):**
   ```bash
   python -m jobscout "test query" --verbose
   ```

### Code Style

- **Python**: Follow PEP 8, use type hints, max line length 100
- **TypeScript**: Use strict mode, prefer functional components
- **Formatting**: No enforced formatter (yet), but be consistent
- **Imports**: Group imports (stdlib, third-party, local)

---

## Common Tasks

### Adding a New Job Provider

1. Create `jobscout/providers/{name}.py`:
   ```python
   from jobscout.providers.base import Provider
   from jobscout.models import NormalizedJob, Criteria
   
   class MyProvider(Provider):
       name = "myprovider"
       
       async def collect(self, fetcher, criteria):
           # Fetch jobs from API/RSS/scraping
           # Return List[NormalizedJob]
           pass
   ```

2. Register in `jobscout/providers/__init__.py`

3. Add to `jobscout/orchestrator.py` provider list

### Fixing Provider Errors

- Check `provider.stats.error_messages` in logs
- Common issues:
  - API endpoint changed
  - HTML structure changed
  - Rate limiting
  - Network timeouts

### Adding a New API Endpoint

1. Create/update `backend/app/api/{name}.py`
2. Add router to `backend/app/main.py`:
   ```python
   from backend.app.api import {name}
   app.include_router({name}.router, prefix=settings.api_prefix)
   ```

### Updating Frontend Components

- Components are in `frontend/components/`
- Use Tailwind CSS for styling
- Follow existing patterns (minimal, clean design)

### Debugging Scrape Issues

1. Check Fly.io logs: `fly logs -a jobscout-api`
2. Check run status: `GET /api/v1/runs/{run_id}`
3. Check provider stats in orchestrator logs
4. Test provider individually in CLI

---

## Deployment

> **Note**: See [Quick Start & Commands](#quick-start--commands) for exact deploy commands.

### Backend (Fly.io)

**Deploy:**
```bash
fly deploy -a jobscout-api
```

**Set secrets (if needed):**
```bash
fly secrets set JOBSCOUT_DATABASE_URL="..." -a jobscout-api
fly secrets set JOBSCOUT_CORS_ORIGINS='["https://jobscoutai.vercel.app"]' -a jobscout-api
fly secrets set JOBSCOUT_ADMIN_TOKEN="..." -a jobscout-api
```

**Check logs:**
```bash
fly logs -a jobscout-api
```

**Important**: 
- Uses `backend/Dockerfile` for build
- Auto-scales based on traffic
- Scheduled scrapes run via APScheduler

### Frontend (Vercel)

**Auto-deploy**: Pushes to `main` branch automatically trigger deployment

**Manual deploy:**
```bash
cd frontend
npx vercel --prod
```

**Environment variables**: Set in Vercel dashboard
- `NEXT_PUBLIC_API_URL=https://jobscout-api.fly.dev/api/v1`

### Database (Supabase)

1. Create project at supabase.com
2. Run SQL schema (see `DEPLOY.md` or `backend/app/storage/postgres.py`)
3. Get connection string from Settings â†’ Database
4. **Use Session pooler connection string** (not Direct) for Fly.io compatibility

---

## Known Issues & TODOs

### Current Issues

1. **Job descriptions formatting**: âœ… Fixed with `FormattedDescription` component
2. **Provider errors**: Some providers (Arbeitnow) may have parsing issues - check logs
3. **CORS**: Ensure all frontend domains are in `JOBSCOUT_CORS_ORIGINS`

### Future Enhancements

1. **Better description parsing**: Use `extract_text_structured()` instead of `strip_html()` to preserve more structure
2. **Email notifications**: Alert users when new jobs match their saved searches
3. **User accounts**: Save favorite jobs, search history
4. **More providers**: Add more job boards (Indeed, LinkedIn, etc.)
5. **Better AI prompts**: Fine-tune ranking/classification prompts
6. **Export features**: CSV/Excel export from UI
7. **Job alerts**: Email/Slack notifications for new matches

### Technical Debt

1. **Error handling**: More graceful degradation when providers fail
2. **Caching**: Add Redis for job list caching
3. **Testing**: Add unit tests for providers, deduplication
4. **Monitoring**: Add Sentry/error tracking
5. **Rate limiting**: More sophisticated rate limiting (per-user, per-query)

---

## Quick Reference

### Key Files to Modify

- **Add provider**: `jobscout/providers/{name}.py`
- **Change API**: `backend/app/api/{name}.py`
- **Update UI**: `frontend/components/{Component}.tsx`
- **Fix scraping**: `jobscout/orchestrator.py`
- **Database changes**: `backend/app/storage/postgres.py`

### Important Functions

- `run_scrape()`: Main orchestration (`jobscout/orchestrator.py`)
- `enqueue_scrape_run()`: Background scrape trigger (`backend/app/worker.py`)
- `upsert_job_from_dict()`: Save job to DB (`backend/app/storage/postgres.py`)
- `DedupeEngine.dedupe()`: Remove duplicates (`jobscout/dedupe.py`)

### Common Commands

```bash
# Backend logs
fly logs -a jobscout-api

# Test API
curl https://jobscout-api.fly.dev/api/v1/jobs?q=engineer

# Trigger scrape
curl -X POST https://jobscout-api.fly.dev/api/v1/scrape \
  -H "Content-Type: application/json" \
  -d '{"query":"engineer","location":"Remote"}'

# Local CLI scrape
python -m jobscout "engineer" --verbose
```

---

## Contact & Resources

- **Repository**: https://github.com/binary-exe/jobscoutai
- **Backend API**: https://jobscout-api.fly.dev/docs
- **Frontend**: https://jobscoutai.vercel.app
- **Deployment Guide**: See `DEPLOY.md`

---

**Last Updated**: 2026-01-09
**Version**: 1.0.0
